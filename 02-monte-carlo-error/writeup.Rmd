---
title: "Monte Carlo Error"
output: html_document
---

### Donald Kane
### September 13, 2021

# Introduction

  When making decisions in life, we always weigh the odds. Whether it is something that you can actually measure, like gambling, or something you do not know the odds of at all, there is always a probability involved with that event. When finding the true probability of something, replication is key. If something has a 50% chance of happening, it does not always occur 50% of the time, but if you are able to replicate the event many times, it will become closer to happening half the time while reducing the amount of error.

# Background

  The Monte Carlo method approximates results and is used to show the relationship between actual probability and the simulation's relative and absolute errors. It is a way of proving the law of large numbers because as the amount of simulations increase, both errors decrease towards their true values.

# Methods

### Create Data Frame

First, we need to load in our packages:

```{r, message=FALSE}
library(ggplot2)
library(dplyr)
```

  First step, we need to create a data frame to store the values of N, P, absolute error, and relative error.

| Variable | Description 
|:---:|:---:|
|  N  |   # or replicates 
|  P  |   Probability 
|  abs_error  |   p-hat - p 
|  rel_error  |   p-hat - p / p 


```{r}
# Create data frame

output <- expand.grid(
  N = 2^c(2:15),
  P = c(0.01, 0.05, 0.10, 0.25, 0.50),
  abs_error = NA,
  rel_error = NA,
  KEEP.OUT.ATTRS = FALSE
)
```

### Create For Loop

  Next, we need to create a loop to run the simulations. In doing this, we are able to fill in the values for the absolute and relative errors to our data frame. This will allow us to have a complete data frame to be able to graph our results to show that as the number of replicates increases, both errors will diminish.

``` {r}
# For loop to run simulations
r <- 10000
for(i in 1:nrow(output)) {
  p <- output$P[i]
  n <- output$N[i]
  p_hat <- rbinom(r,n,p)/n
  output[i, "abs_error"] <- mean(abs(p_hat - p)) 
  output[i, "rel_error"] <- mean(abs(p_hat-p)/p) 
}
```

Just to show that our loop worked, here we used the head() function to show the first 5 rows.

``` {r}
head(output)
```

# Results

`install.packages("remotes")`
`remotes::install_github("thomasgstewart/tgsify")`
```{r, message=FALSE}
library(tgsify)
```


### Relative Error

  Relative error is defined as the absolute value of the observed probability we got from the simulation (p-hat) minus the actual probability and then divided by the actual probability. Relative probability can be better explained as the percent difference in actual vs. observed ((new-old)/old)x100. So for relative probability, we want to see, in percent terms, how far off our p-hat is from the actual probability as replicates increase.

``` {r}
output %>%
  mutate(x = log2(N)) %>%
  mutate(col = as.factor(P) %>% as.numeric) %>%
  plot_setup(rel_error ~ x, c(0,15)) %>%
  split(.$P) %>%
  lwith({
    lines(x, rel_error, col = col[1], lwd = 5, type = "b", pch = 16)
    text(x[1], rel_error[1], "p="%|%P[1], pos = 2, col = col[1])
  })
axis(2)
axis(1, at = axTicks(1), labels = 2^axTicks(1))
box()

```

### Absolute Error

  Absolute error is more easily understood than relative error. It is simply the absolute value of the difference of our observed probability from the simulations and the true probability. With this value, we want to see how far our observed probability is from its true probability.

``` {r}
output %>%
  mutate(x = log2(N)) %>%
  mutate(col = as.factor(P) %>% as.numeric) %>%
  plot_setup(abs_error ~ x, c(0,15)) %>%
  split(.$P) %>%
  lwith({
    lines(x, abs_error, col = col[1], lwd = 5, type = "b", pch = 16)
    text(x[1], abs_error[1], "p="%|%P[1], pos = 2, col = col[1])
  })
axis(2)
axis(1, at = axTicks(1), labels = 2^axTicks(1))
box()
```

# Conclusions

The Monte Carlo experiment is used to approximate results, but in this experiment we took it one step further and observed the error. We wanted to see how changing the number of replicates would impact our relative and absolute errors. Given that we used 5 different probabilities, we were able to show how increasing the number of replicates would "squeeze" our graphs to a "near zero" error margin between the observed probability and actual probability due to the Law of Large Numbers.